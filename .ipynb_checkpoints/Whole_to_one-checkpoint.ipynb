{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00157250",
   "metadata": {},
   "source": [
    "# The requirements before start\n",
    "```conda install lxml\n",
    "pip install pystemmer\n",
    "conda install requests```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854bcb2",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "\n",
    "URL = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract.xml.gz'\n",
    "with requests.get(URL, stream=True) as req:\n",
    "    req.raise_for_status()\n",
    "    with open('./enwiki-latest-abstract.xml.gz', 'wb') as f:\n",
    "        for i, chunk in enumerate(req.iter_content(chunk_size=1024*1024)):\n",
    "            f.write(chunk)\n",
    "            if i%10 == 0:\n",
    "                print('Downloaded {} mbs'.format(i), end='\\r')\n",
    "                    \n",
    "               \n",
    "@dataclass\n",
    "class Abstract:\n",
    "    ID : int\n",
    "    title : str\n",
    "    abstract : str\n",
    "    url : str\n",
    "        \n",
    "    @property\n",
    "    def fulltext(self):\n",
    "        return ' '.join([self.title, self.abstract])\n",
    "\n",
    "abstract_result = []\n",
    "with gzip.open('./enwiki-latest-abstract.xml.gz' 'rb') as f:\n",
    "    doc_id = 1\n",
    "    \n",
    "    for _, element in etree.iterparse(f, events=('end',), tag='doc'):\n",
    "        print(_, element)\n",
    "        title = element.findtext('./title')\n",
    "        url = element.findtext('./url')\n",
    "        abstract = element.findtext('./abstract')\n",
    "        abstract_result.append(Abstract(ID=doc_id, title=title, url=url, abstract=abstract))\n",
    "        doc_id += 1\n",
    "        element.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b869e9",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1141c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Stemmer\n",
    "import string\n",
    "\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "stopwords = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n",
    "                 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n",
    "                 'do', 'at', 'this', 'but', 'his', 'by', 'from', 'wikipedia'])\n",
    "punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "def analyze(text):\n",
    "    tokens = test.split()\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [punctuation.sub('', token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in st0opwords]\n",
    "    tokens = [stemmer.stemWords(tokens)]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "class Index:\n",
    "    def __init__(self):\n",
    "        self.index = {}\n",
    "        self.documents = {}\n",
    "        \n",
    "    def index_document(self, document):\n",
    "        if document.ID not in self.documents:\n",
    "            self.documents[document.ID] = document\n",
    "        for token in analyze():\n",
    "            if token not in self.index:\n",
    "                self.index[token] = set()\n",
    "            self.index[token].add(document.ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
